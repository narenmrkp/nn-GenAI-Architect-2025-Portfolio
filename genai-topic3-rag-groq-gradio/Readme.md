ğŸ§  Topic-3: Retrieval-Augmented Generation (RAG)
-------------------------------------------------------------
ğŸ“… Status: In Progress
ğŸ”— Models: LLaMA3 (via Groq API)
ğŸ§° Tools: LangChain, Chroma, Wikipedia, Gradio

ğŸ“˜ Brief Theory Summary
Retrieval-Augmented Generation (RAG) is a technique where an LLM combines external knowledge retrieval with its generation capability.

ğŸ” Why RAG?
LLMs are not always accurate on niche topics
With RAG, we can provide real documents, PDFs, or live web content
This improves accuracy, factuality, and customization
----------------------------------------------------------------
ğŸ§© RAG Components:
Component	Description
Retriever	Searches for relevant docs (PDF, DB, Wikipedia, etc.)
Vector Store	Converts documents to vectors and indexes them
LLM	Uses retrieved context to generate final response
Chain	Combines steps (retrieval â†’ answer generation)

ğŸ› ï¸ Projects Overview (All with Groq + Gradio)
##################################################################################################################################
âœ… Project 1: RAG from Wikipedia using LangChain
ğŸ¯ Goal: Answer any query using real-time Wikipedia-based search (no hallucination).

ğŸ” Features:
User enters question
LangChain fetches relevant page content using WikipediaRetriever
Groq LLM uses the context to give accurate answers
-------------------------------------------------------------------
ğŸ“š Concepts Learned:
Wikipedia search retrieval
Simple RAG chain using stuff method
Dynamic context injection
----------------------------------------------------------------------------------------------------------
ğŸ§  Resume Line:
Built a real-time Wikipedia-powered RAG pipeline with Groqâ€™s LLaMA3 and Gradio UI, ensuring accurate, live knowledge queries.
###########################################################################################################################
âœ… Project 2: RAG from Uploaded PDF using Chroma Vector DB
ğŸ¯ Goal: Ask questions from your custom documents.

ğŸ” Features:
Upload any PDF
Chunks split and embedded using Groq LLM
Stored in Chroma Vector DB
User can ask questions â†’ RAG answers from that document only

ğŸ“š Concepts Learned:
FAISS/Chroma Vector DB
PDF chunking and embedding
File-based retrieval + QA chain

ğŸ§  Resume Line:
Created a local document RAG engine using Chroma vectorstore and Groq API to answer queries from uploaded PDFs.
##################################################################################################################################
âœ… Project 3: Hybrid RAG â€“ Combine PDF + Wikipedia
ğŸ¯ Goal: Use multi-retriever setup to pull context from both:

Uploaded PDF
Wikipedia (real-time)

ğŸ” Features:
Smart switch: PDF + Wiki combined context
Final answer uses merged facts
All Groq-based, no OpenAI or SerpAPI

ğŸ“š Concepts Learned:
MultiRetriever logic
Context merger
Chain routing

ğŸ§  Resume Line:
Developed a hybrid RAG system that blends real-time Wikipedia and user-uploaded documents into a single answer pipeline using Groq LLaMA3 and LangChain.
##################################################################################################################################
âœ… Mastered Concepts from Topic-3
Concept	Covered? âœ…
WikipediaRetriever	âœ… Project 1
PDF VectorStore (Chroma)	âœ… Project 2
LangChain QA Chains	âœ… All
Groq LLM in RAG	âœ… All
Multi-Retriever Routing	âœ… Project 3
Contextual Answer Generation	âœ… All
############################################################################# End of Topic-3 (Projects Completed Successfully) #####################################################
