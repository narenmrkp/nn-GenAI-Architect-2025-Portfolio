ğŸ”§ Module-5: Model Tuning & Ensemble Learning
ğŸ¯ Goal: Improve accuracy by optimizing hyperparameters and combining models.

ğŸ§  ML Module-5: Hyperparameter Tuning & Ensemble Learning â€“ Diabetes Prediction ğŸ§ª
ğŸ” Optimizing classification performance using GridSearchCV, Cross-Validation, and Ensemble Models like Voting & Stacking.

ğŸ“Œ Project Overview
This project focuses on improving prediction accuracy using advanced machine learning techniques:

âœ… Hyperparameter tuning with GridSearchCV
âœ… Cross-validation to reduce overfitting
âœ… Powerful ensemble models like:
  ğŸ¤ VotingClassifier (soft voting)
  ğŸ”— StackingClassifier (meta-learning)
âœ… Model Evaluation via:
  Accuracy, Precision, Recall, F1-Score
  ROC-AUC and Confusion Matrix

ğŸ“ Dataset Used
ğŸ“„ Pima Indians Diabetes Dataset
Source: UCI / Brownlee GitHub
Total Records: 768 | Features: 8 | Target: Outcome (0/1)

âš™ï¸ Models Trained & Tuned:
| ğŸ”§ Model             | â¬†ï¸ Tuning Method      | ğŸ§  Description                          |
| -------------------- | --------------------- | --------------------------------------- |
| `SVC` (SVM)          | GridSearchCV + CV(5)  | Finds best `C`, `kernel`                |
| `Random Forest`      | GridSearchCV + CV(5)  | Tunes `n_estimators`, `max_depth`       |
| `VotingClassifier`   | Soft voting ensemble  | Combines SVM, RF, Logistic Regression   |
| `StackingClassifier` | Meta-learner (LogReg) | Combines KNN + DecisionTree base models |

ğŸ“Š Performance Summary:
| Model                 | Accuracy | Precision | Recall | F1-Score | ROC-AUC |
| --------------------- | -------- | --------- | ------ | -------- | ------- |
| âœ… VotingClassifier    | `0.79`   | `0.77`    | `0.74` | `0.75`   | `0.83`  |
| ğŸ”— StackingClassifier | `0.78`   | `0.76`    | `0.73` | `0.74`   | `0.81`  |
| SVM (Tuned)           | `0.76`   | `0.75`    | `0.71` | `0.73`   | `0.80`  |
| RF (Tuned)            | `0.75`   | `0.74`    | `0.69` | `0.71`   | `0.78`  |

ğŸ“Œ Best performance achieved with VotingClassifier!

ğŸ“ˆ Visualizations (Uploaded)
ğŸ” Metric	ğŸ“¸ Screenshot
Confusion Matrix â€“ SVM	
Confusion Matrix â€“ RF	
Confusion Matrix â€“ Voting	
Confusion Matrix â€“ Stacking	
ROC Curve Comparison	
Model Comparison Table	

ğŸ’¾ Saved Models
All models and scaler are saved as .pkl files:
models/
â”œâ”€â”€ svm_tuned.pkl
â”œâ”€â”€ rf_tuned.pkl
â”œâ”€â”€ voting_model.pkl
â”œâ”€â”€ stack_model.pkl
â”œâ”€â”€ scaler.pkl
Use this to reload models for deployment or inference.

ğŸš€ Tech Stack
scikit-learn
GridSearchCV, VotingClassifier, StackingClassifier
pandas, matplotlib, seaborn
.pkl file saving for production

ğŸ How to Run
# Clone this repo or open in Colab
!pip install scikit-learn
!python Diabetes_Tuned_Classification.ipynb

ğŸ¤ Author
Nandyala Narendra
ğŸ”— GitHub | ğŸ”— LinkedIn

ğŸ“¢ Recruiter Note
This project demonstrates my expertise in:
  Practical model improvement using tuning
  Model selection based on multiple metrics
  Ensemble learning & robust evaluation
  Deployable models using .pkl files
